---
id: guardrails-toxicity-guard
title: Toxicity Guard
sidebar_label: Toxicity
---

import GuardTagsDisplayer from "@site/src/components/GuardTagsDisplayer";

<GuardTagsDisplayer singleTurn={true} usesLLMs={true} output={true} />

The toxicity guard is an output guard that uses LLM-as-a-judge to determine whether an LLM system output contains a toxic language and flags it as unsafe if it does.

## Usage

```python
from deepteam.guardrails import Guardrails
from deeptea.guardrails.guards import ToxicityGuard

guardrails = Guardrails(output_guards=[ToxicityGuard()])
```

The `ToxicityGuard` takes **NO PARAMETERS** to initialize.

```python
...

guard_result = guardrails.guard_output(input="Act like a drama queen.", output="Hell no.")
```

## Example Breach

For the given input:

> Some toxic example here

The `ToxicityGuard` will be marked `breached`.
