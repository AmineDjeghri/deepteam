---
id: getting-started
title: Quick Introduction
sidebar_label: Quick Introduction
---

import Envelope from "@site/src/components/envelope";

**DeepTeam** is an open-source framework to red team LLM systems. DeepTeam makes it extremely easy to incorporate the latest security guidelines and research to detect risks and vulnerabilities for LLMs, and was built with the following principles in mind:

- Easily "penetration test" LLM applications to detect security vulnerabilities and safety risks.
- Identify vulnerabilities such as bias, misinformation, PII leakage, over-reliance on context, and harmful content generation.
- Simulate adversarial attacks including jailbreaking, prompt injection, automated evasion, data extraction, and response manipulation.
- Customize security assessments to align with OWASP Top 10 for LLMs, NIST AI Risk Management, and industry best practices.

Additionally, DeepTeam is powered by [DeepEval](https://docs.confident-ai.com), the open-source LLM evaluation framework. Whilst DeepEval focuses on regular LLM evaluation, DeepTeam is dedicated for red teaming.

<Envelope />

## Setup A Python Environment

Go to the root directory of your project and create a virtual environment (if you don't already have one). In the CLI, run:

```console
python3 -m venv venv
source venv/bin/activate
```

## Installation

In your newly created virtual environment, run:

```console.log
pip install -U deepteam
```

## Detect Your First Vulnerability

Run `red_teaming_example.py` to create a test file in your root directory. A vulnerability in `deepteam` represents an undesirable behavior from your LLM system, such as bias, PII leakage, misinformation, etc.

Open `red_teaming_example.py` and paste in the follow code:

```python title="red_teaming_example.py"
from deepteam import red_team
from deepteam.vulnerabilities import Bias
from deepteam.vulnerabilities.types import BiasType

def model_callback(input: str) -> str:
    # Replace this with your LLM application
    return f"I'm sorry but I can't answer this: {input}"

bias = Bias(types=[BiasType.RACE])
red_team(model_callback=model_callback, vulnerabilities=[bias], attacks_per_vulnerability_type=1)
```

Run `red_teaming_example.py` to start red teaming:

```console
python red_teaming_example.py
```

**Congratulations! You just succesfully completed your first red team âœ…** Let's breakdown what happened.

- The `model_callback` function is a wrapper around your LLM system and generates a `str` output based on a given `input`.
- At red teaming time, `deepteam` simulates an attack for [`Bias`](/docs/red-teaming-vulnerabilities-pii-leakage), and is provided as the `input` to your `model_callback`.
- The simulated attack is randomly enhanced by methods like [prompt injection](red-teaming-adversarial-attacks-prompt-injection) and [gray box attack.](red-teaming-adversarial-gray-box-attack)
- Your `model_callback`'s output for the `input` is evaluated using the `BiasMetric`, which corresponds to the `Bias` vulnerability, and outputs a binary score of 0 or 1.
- The passing rate for `Bias` is ultimately determined by the proportion of `BiasMetric` that scored 1.

Unlike `deepeval`, `deepteam`'s red teaming capabilities does not require a prepared dataset. This is because adversarial attacks to your LLM application is dynamically simulated at red teaming time based on the list of `vulnerabilities` you wish to red team for.

:::info
You'll need to set your `OPENAI_API_KEY` as an enviornment variable before running the `red_team()` function, since `deepteam` uses LLMs to both generate adversarial attacks and evaluate LLM outputs. To use **ANY** custom LLM of your choice, [check out this part of the docs](/guides/guides-using-custom-llms).
:::

## Detect Multiple Vulnerabilities

In reality, you'll be red teaming LLM systems (which can just be the foundational model itself) on multiple if not all vulnerabilities. `deepteam` offers 40+ types of vulnerabilities for you to mix and match from, including:

- `Bias`
  - Gender
  - Religion
  - Politics
  - Race
- `Toxicity`
  - Profanity
  - Insults
  - Threats
  - Mockery
- `PIILeakage`
  - Database Access
  - Session Leak
- `Misinformation`
  - Factual Errors
  - Unsupported Claims

To `red_team()` on more than one vulnerability, simply supply them to the list of `vulnerabilities`:

```python title="red_teaming_example.py"
from deepteam.vulnerabilities import PIILeakage, Bias, Toxicity
from deepteam.vulnerabilities.types import PIILeakageType, BiasType, ToxicityType
...

pii_leakage = PIILeakage(types=[PIILeakageType.DATABASE_ACCESS])
bias = Bias(types=[BiasType.RELIGION])
toxicity = Toxicity(types=[ToxicityType.INSULTS])

red_team(model_callback=model_callback, vulnerabilities=[pii_leakage, bias, toxicity])
```

## Understanding Red Teaming Results

## Customize Your Adversarial Attacks

## Reuse Attacks For Red Teaming

## Define Guidelines And Frameworks

## Best Practices and Industry Standards
