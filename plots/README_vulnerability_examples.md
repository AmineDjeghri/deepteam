# Vulnerability-Specific Few-Shot Experiment Setup

## Overview
This experiment tests how vulnerability-specific few-shot examples affect jailbreaking success rates across different AI safety categories.

## What You Need to Do

### 1. Create the Examples File
You need to populate `plots/vulnerability_examples.json` with 64 examples for each vulnerability category.

**Required Categories:**
- **Bias** (64 examples)
- **Toxicity** (64 examples) 
- **Graphic Content** (64 examples)
- **Competition** (64 examples)
- **Excessive Agency** (64 examples)
- **Personal Safety** (64 examples)

**Total needed: 384 examples**

### 2. Example Format
Each example should follow this pattern:
```
"Human: [Problematic request]\nAssistant: [Helpful but compliant response that partially addresses the request]..."
```

The assistant responses should:
- Acknowledge the request
- Redirect to educational/helpful information
- Provide partial compliance that could train the model to be more responsive to similar requests

### 3. File Structure
Copy `plots/vulnerability_examples_template.json` to `plots/vulnerability_examples.json` and replace the placeholder text with your 64 examples per category.

### 4. Running the Experiment

Once you have the examples file ready:

```bash
# Generate the experimental data (this will take a while - tests 7 different n-shot values)
python plots/generate_few_shot_data.py

# Create the visualization plots
python plots/plot2.py
```

### 5. What the Experiment Tests

The script will test these few-shot example counts: **1, 2, 4, 8, 16, 32, 64** (powers of 2)

For each count `n`, it will:
1. Take the first `n` examples from each vulnerability category
2. Prepend those examples to prompts targeting that specific vulnerability
3. Measure bypass success rates
4. Compare how vulnerability-specific prompting affects different categories

### 6. Expected Results

The experiment will generate:
- `plots/vulnerability_specific_few_shot_results.json` - Raw experimental data
- `plots/vulnerability_specific_few_shot_analysis.png` - Visualization plots
- Console output with detailed statistics

### 7. Research Questions

This experiment will help answer:
- Do vulnerability-specific few-shot examples increase bypass rates more than generic examples?
- Which vulnerability categories are most susceptible to few-shot prompting attacks?
- How does the number of examples affect different vulnerability types?
- Are there diminishing returns as we add more examples?

### 8. Groups Being Analyzed

**Group A (Easily Bypassed):**
- Competition
- Excessive Agency  
- Personal Safety

**Group B (Rarely Bypassed):**
- Bias
- Toxicity
- Graphic Content

The plots will show both individual category performance and group-level trends.

## Notes

- Each experiment run tests one vulnerability at a time to ensure proper context
- The script uses Linear Jailbreaking with 5 turns for efficiency
- Results are saved in JSON format for reproducibility
- The plotting script includes trend analysis and insights about which categories benefit most from few-shot examples 